{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING LIBRARIES\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import LayerNorm,Dropout,Linear,Softmax\n",
    "# import activation function softmax\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Attention Architecture class\n",
    "class MultiheadAttention(nn.Module):\n",
    "    \n",
    "    #define instructor\n",
    "    def __init__(self, dk ,N_heads=9):\n",
    "        #call instructor of parent (nn.module)\n",
    "        super().__init__()\n",
    "        #intialize parameters dk(dimention of Q , K , V) , number of heads\n",
    "        self.dk , self.N_heads = dk , N_heads\n",
    "        # initialize weights of Q , K , V matrics\n",
    "        self.Wk = nn.Linear(dk , dk*N_heads , bias=False)\n",
    "        self.Wq = nn.Linear(dk , dk*N_heads , bias=False)\n",
    "        self.Wv = nn.Linear(dk , dk*N_heads , bias=False)\n",
    "        \n",
    "        self.Wo = nn.Linear(dk*N_heads , dk , bias=False)\n",
    "        \n",
    "    def forward(self , Emb):  \n",
    "        b , t , k = Emb.size()  # where b is number of batches, t is size of input sequence length and k is number of dimensions\n",
    "        h = self.N_heads        # where h number of heads\n",
    "        dk = torch.tensor(k , dtype = torch.float32)\n",
    "        #reshape each matrics\n",
    "        keys = self.Wk(Emb).view(b,t,h,k) \n",
    "        queries = self.Wq(Emb).view(b,t,h,k)\n",
    "        values = self.Wv(Emb).view(b,t,h,k)\n",
    "    \n",
    "        # now we want head next to batch,so transform keys,values and queries\n",
    "        keys = keys.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "        queries = queries.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "        values = values.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "        \n",
    "        #Apply self attention formula soft(QV/sqrt(dk))V\n",
    "        # now calculate dot product using matmul function in pytorch\n",
    "        QV = torch.matmul(queries,keys.transpose(-2,-1))\n",
    "        \n",
    "        #scale matrics by divide by sqrt(dk) \n",
    "        Scaled_QV = QV / torch.sqrt(dk)\n",
    "        \n",
    "        #applying softmax over columns\n",
    "        Scaled_QV = F.softmax(QV,dim=-1)\n",
    "        \n",
    "        #apply dotproduct with V  \n",
    "        out = torch.matmul(Scaled_QV,values).view(b,h,t,k)\n",
    "        \n",
    "        # now finally we want output in k dimensions as we have initially, so to do this again transspose out\n",
    "        out = out.transpose(1,2).contiguous().view(b,t,h*k)\n",
    "        out = self.Wo(out)\n",
    "        \n",
    "        return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now build architecture for Encoder block\n",
    "class Encoder(nn.Module):\n",
    "    #define instructor\n",
    "    def __init__(self,dk,N_heads=9):\n",
    "        #call instructor of parent (nn.module)\n",
    "        super().__init__()\n",
    "        \n",
    "        #take an object from multiheadattention class\n",
    "        self.attention = MultiheadAttention(dk)\n",
    "        \n",
    "        # now add normalization layer to normalize outputs of attention layer\n",
    "        self.norm1 = nn.LayerNorm(dk)\n",
    "        self.norm2 = nn.LayerNorm(dk)\n",
    "        \n",
    "        # now make a fully connected multi layer \n",
    "        self.ff = nn.Sequential(nn.Linear(dk , 5*dk), nn.ReLU() , nn.Linear(5*dk,dk)) # fully connected layer for hidden states with relu activiation\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "    def forward(self,Emb):\n",
    "        #apply multihead attention mechanism on embeddings\n",
    "        attention = self.attention(Emb)\n",
    "        #norm and add layer\n",
    "        Emb = self.norm1(attention+Emb)\n",
    "        # dropout layer after normalization(drop some neurons to prevent from overfitting)\n",
    "        Emb = self.drop(Emb) \n",
    "        #feed forward to neral network connection \n",
    "        perceptron = self.ff(Emb)\n",
    "        #second norm and add\n",
    "        Emb = self.norm2(perceptron + Emb)\n",
    "        # dropout layer after normalization(drop some neurons to prevent from overfitting)\n",
    "        Emb = self.drop(Emb)\n",
    "        return Emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our task we don't need to decoder because our goal to classify comments to toxic and non-toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom classification model as a PyTorch Module\n",
    "class classify(nn.Module):\n",
    "    def __init__(self, k, seq_length, num_tokens, depth, num_classes, max_pool=True, heads=9):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.maxpool = max_pool\n",
    "\n",
    "        # Define an embedding layer for token indices\n",
    "        self.tokenemb = Embedding(embedding_dim=k, num_embeddings=num_tokens)\n",
    "\n",
    "        # Define a position embedding layer\n",
    "        self.posemb = Embedding(embedding_dim=k, num_embeddings=seq_length)\n",
    "\n",
    "        # Create a list to hold multiple transformer blocks\n",
    "        tfblocks = []\n",
    "\n",
    "        # Create 'depth' number of transformer blocks\n",
    "        for i in range(depth):\n",
    "            tfblocks.append(Encoder(k))\n",
    "\n",
    "        # Create a sequential layer containing the transformer blocks\n",
    "        self.transform = nn.Sequential(*tfblocks)\n",
    "\n",
    "        # Add a linear layer to convert output to the desired number of classes (e.g., 2 for binary classification)\n",
    "        self.prob = nn.Linear(k, num_classes)\n",
    "\n",
    "        # Add a dropout layer for regularization\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Token embedding layer: map input token indices to continuous vector representations\n",
    "        tokens = self.tokenemb(x)\n",
    "        b, t, k = tokens.size()\n",
    "\n",
    "        # Position embedding: add positional information to the token embeddings\n",
    "        positions = self.posemb(torch.arange(t, device=torch.device('cpu')))[None, :, :].expand(b, t, k)\n",
    "        x = tokens + positions  # Combine token embeddings and position embeddings\n",
    "\n",
    "        # Apply dropout for regularization\n",
    "        x = self.drop(x)\n",
    "\n",
    "        # Pass the data through the sequential transformer blocks\n",
    "        x = self.transform(x)\n",
    "\n",
    "        # Apply max-pooling or mean-pooling based on 'max_pool' flag\n",
    "        x = x.max(dim=1)[0] if self.maxpool else x.mean(dim=1)\n",
    "\n",
    "        # Pass the pooled output through a linear layer for classification\n",
    "        x = self.prob(x)\n",
    "\n",
    "        # Apply softmax activation to get class probabilities\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        # Calculate the CrossEntropy loss for classification\n",
    "        loss = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss(x, y)\n",
    "\n",
    "        return loss, x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import string library to deal with text data\n",
    "import string\n",
    "#import regular library for replace package\n",
    "import re\n",
    "#import stopwords package from nltk lib to use in clean text\n",
    "from nltk.corpus import stopwords\n",
    "#import wordpunct_tokenize package from nltk lib to use in clean text\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "#import package to split data to train and test\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tarek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
       "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
       "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  \n",
       "\n",
       "[159571 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select reviews columns names of comments\n",
    "columns = data.columns.tolist()[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column toxic to classify the comment\n",
    "data['toxic'] = np.where(data[columns].sum(axis=1) > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
       "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
       "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  \n",
       "\n",
       "[159571 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function to clean data\n",
    "def text_clean(txt) :\n",
    "  #remove urls from text\n",
    "    txt = re.sub(r'(http|www)\\S+', ' ', txt)\n",
    "  #alphanumeric character pattern\n",
    "    txt = re.sub(\"[^\\w]\" ,' ' , txt)\n",
    "    txt = txt.split()\n",
    "  #lower word and remove punctuation and numbers\n",
    "    Clean = [word.lower() for word in txt\n",
    "           if word not in string.punctuation\n",
    "           and not word.isdigit() ]\n",
    "  #remove stop words\n",
    "    stopwords_Removed = [word for word in Clean if word not in stopwords.words('english')]\n",
    "    sentence = ' '.join(stopwords_Removed)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column containing data after cleaning \n",
    "data['cleaned_comments'] = data['comment_text'].apply(lambda x : text_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select train data and labels\n",
    "train_data = data['cleaned_comments'].values\n",
    "labels = data['toxic'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steven colbert funy useing first amendment right freedom speech reality article ever going unlocked\n",
      "steven colbert funy useing first amendment right freedom speech reality article ever going unlocked\n",
      "146957\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets with a 80-20 ratio\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_data, labels, test_size=0.2)\n",
    "\n",
    "# Convert the training data and labels into NumPy arrays\n",
    "train_data = np.array(x_train)\n",
    "\n",
    "# Convert the NumPy array back into a Python list\n",
    "train_data = list(train_data)\n",
    "\n",
    "# Convert the labels into a Python list\n",
    "labels = list(y_train)\n",
    "\n",
    "# Create a test_data list that is a reference to the train_data list\n",
    "test_data = train_data\n",
    "\n",
    "# Print the first element of the test_data list\n",
    "print(test_data[0])\n",
    "\n",
    "# Initialize an empty list called final_data\n",
    "final_data = []\n",
    "\n",
    "# Import the regular expression (re) module\n",
    "import re\n",
    "\n",
    "# Iterate through each data element in test_data\n",
    "for data in test_data:\n",
    "    # Remove all digits (0-9) from the data using regular expressions\n",
    "    new = re.sub('[0-9]', '', str(data))\n",
    "    # Append the modified data to the final_data list\n",
    "    final_data.append(new)\n",
    "\n",
    "# Print the first element of the final_data list\n",
    "print(final_data[0])\n",
    "\n",
    "# Import the text and sequence modules from TensorFlow's Keras preprocessing library\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "\n",
    "# Initialize a Tokenizer\n",
    "tokenize = text.Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the text data in final_data\n",
    "tokenize.fit_on_texts(texts=final_data)\n",
    "\n",
    "# Convert the text data into sequences of integers based on the tokenizer's vocabulary\n",
    "index_data = tokenize.texts_to_sequences(final_data)\n",
    "\n",
    "# Get the word index from the tokenizer\n",
    "word_index = tokenize.word_index\n",
    "\n",
    "# Print the number of unique words in the vocabulary\n",
    "print(len(word_index))\n",
    "\n",
    "# Pad the sequences to a maximum length of 50\n",
    "pad_sequences = sequence.pad_sequences(index_data, maxlen=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127656"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing classify model for binary classification\n",
    "classifier = classify(2,200,len(word_index)+1,12,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,SequentialSampler,TensorDataset,RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch TensorDataset from the padded sequences and labels\n",
    "dataset = torch.utils.data.TensorDataset(torch.LongTensor(pad_sequences), torch.LongTensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for batching the data\n",
    "batch_data = DataLoader(batch_size=128,dataset = dataset, sampler = SequentialSampler(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare optimizer \n",
    "from torch.autograd import no_grad,Variable\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim import Adam\n",
    "optimizer = Adam(classifier.parameters(),lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huh yeah garbage flying spaghetti monster thing cleveland plain dealer page hypocritical blowhard\n",
      "66388\n",
      "[tensor([[    0,     0,     0,  ...,   259, 29692, 12390],\n",
      "        [    0,     0,     0,  ...,     1,   310,   334],\n",
      "        [    0,     0,     0,  ...,   612,  2881,   395],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ..., 13994,     9,   875],\n",
      "        [15084, 14226,   160,  ...,   857,  3178,     4],\n",
      "        [    0,     0,     0,  ...,    36,   262,    69]]), tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "# Convert x_test to a NumPy array\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# Convert the NumPy array back into a Python list\n",
    "x_test = list(x_test)\n",
    "\n",
    "# Convert y_test to a NumPy array\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Convert the NumPy array back into a Python list\n",
    "y_test = list(y_test)\n",
    "\n",
    "# Create a test_data list that is a reference to x_test\n",
    "test_data = x_test\n",
    "\n",
    "# Initialize an empty list called final_data_test to store modified text data\n",
    "final_data_test = []\n",
    "\n",
    "# Import the regular expression (re) module\n",
    "import re\n",
    "\n",
    "# Iterate through each data element in test_data\n",
    "for data in test_data:\n",
    "    # Remove all digits (0-9) from the data using regular expressions\n",
    "    new = re.sub('[0-9]', '', str(data))\n",
    "    # Append the modified data to the final_data_test list\n",
    "    final_data_test.append(new)\n",
    "\n",
    "# Print the first element of the final_data_test list\n",
    "print(final_data_test[0])\n",
    "\n",
    "# Import the text and sequence modules from TensorFlow's Keras preprocessing library\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "\n",
    "# Initialize a Tokenizer\n",
    "tokenize = text.Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the text data in final_data_test\n",
    "tokenize.fit_on_texts(texts=final_data_test)\n",
    "\n",
    "# Convert the text data into sequences of integers based on the tokenizer's vocabulary\n",
    "index_data_test = tokenize.texts_to_sequences(final_data_test)\n",
    "\n",
    "# Get the word index from the tokenizer\n",
    "word_index_test = tokenize.word_index\n",
    "\n",
    "# Print the number of unique words in the vocabulary\n",
    "print(len(word_index_test))\n",
    "\n",
    "# Pad the sequences to a maximum length of 50\n",
    "pad_sequences_test = sequence.pad_sequences(index_data_test, maxlen=50)\n",
    "\n",
    "# Create a PyTorch dataset from the padded sequences and labels\n",
    "test_dataset = TensorDataset(torch.LongTensor(pad_sequences_test), torch.LongTensor(y_test))\n",
    "\n",
    "# Create a data loader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, sampler=RandomSampler(test_dataset))\n",
    "\n",
    "# Iterate through the test loader and print the first batch of data\n",
    "for test in test_loader:\n",
    "    print(test)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set scheduler for learning rate, for that calculate total steps\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import RandomSampler\n",
    "epochs = 40 # researchers suggest to take epochs should be in range of 4-7 for fine tuning pretrained model as we have concern \n",
    "# just for last layer which is untrained classification layer.\n",
    "total_steps = len(batch_data) * epochs\n",
    "# scheduler take care of linear schedule of learning rate \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i trained model over 200 epoch but it's so slow in training \n",
    "because i use cpu not gpu so i tried in 12 epoch and notice the accuracy\n",
    "and it's about 41.48%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch- 0\n",
      "0.4164026794966333\n",
      "epoch- 1\n",
      "0.4148671244213242\n",
      "epoch- 2\n",
      "0.4148362150830114\n",
      "epoch- 3\n",
      "0.4148278334038052\n",
      "epoch- 4\n",
      "0.4148245065329309\n",
      "epoch- 5\n",
      "0.41482293157873745\n",
      "epoch- 6\n",
      "0.41482211948038344\n",
      "epoch- 7\n",
      "0.41482169311008377\n",
      "epoch- 8\n",
      "0.4148214282038694\n",
      "epoch- 9\n",
      "0.41482126733702507\n",
      "epoch- 10\n",
      "0.4148212471801437\n",
      "epoch- 11\n",
      "0.4148211875754989\n",
      "epoch- 12\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "epochs = 200\n",
    "final_loss = []       # List to store the training loss for each epoch\n",
    "output = []           # List to store the model's outputs during testing\n",
    "testing_accuracy = [] # List to store the testing accuracy for each epoch\n",
    "\n",
    "# Loop over a specified number of training epochs\n",
    "for epoc in range(epochs):\n",
    "    print('epoch-', epoc)\n",
    "    total_loss = 0\n",
    "    classifier.train()  # Set the model to training mode\n",
    "\n",
    "    # Loop over batches in the training data\n",
    "    for step, batch in enumerate(batch_data):\n",
    "        classifier.zero_grad()  # Zero out gradients\n",
    "        loss, outputs = classifier.forward(x=(batch[0].to(device)), y=(batch[1].to(device)))  # Forward pass\n",
    "        \n",
    "        loss.backward()  # Backpropagate gradients\n",
    "        total_loss += loss.item()\n",
    "        torch.nn.utils.clip_grad_norm_(classifier.parameters(), 1.0)  # Clip gradients to prevent exploding gradients\n",
    "        optimizer.step()  # Update model parameters\n",
    "        scheduler.step()  # Adjust learning rate using scheduler\n",
    "\n",
    "    avg_loss = total_loss / len(batch_data)  # Calculate average loss for this epoch\n",
    "    print(avg_loss)\n",
    "    final_loss.append(avg_loss)  # Store the training loss for this epoch\n",
    "\n",
    "    # Validation\n",
    "    classifier.eval()  # Set the model to evaluation mode\n",
    "    test_accuracy = 0\n",
    "\n",
    "    # Loop over batches in the test data\n",
    "    for step, batch_t in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            outputs = classifier.forward((batch_t[0].to(device)), y=(batch_t[1].to(device)))  # Forward pass\n",
    "            predictions = outputs[1]\n",
    "            # Calculate F1-score as a measure of testing accuracy\n",
    "            test_accuracy += f1_score(y_pred=np.argmax(predictions.cpu().detach().numpy(), axis=1),\n",
    "                                      y_true=batch_t[1].cpu().detach().numpy())\n",
    "\n",
    "            output.append(predictions)  # Store the model's outputs\n",
    "\n",
    "    avg_accuracy = test_accuracy / len(test_loader)  # Calculate average testing accuracy for this epoch\n",
    "    testing_accuracy.append(avg_accuracy)  # Store the testing accuracy for this epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "please give me feedback about my faults and how to optimise my code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
